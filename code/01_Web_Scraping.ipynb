{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac1a182",
   "metadata": {},
   "source": [
    "# Web Scraping and Data Collection\n",
    "\n",
    "The data collected utilizes primarily data collected from user input regarding factors such as number of players, playtime, language proficiency (how difficult it is to understand instructions), and age recommendations. Through this data, the goal of the project is to create a recommender system that uses this data to recommend similar games. Stretch goals include being able to choose specific characteristics (game category) in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11bb1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import xmltodict\n",
    "import os\n",
    "\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f4c45",
   "metadata": {},
   "source": [
    "## Part 1 - ID and Rankings\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|id|int|ranked_data.csv|Id of the board game.|\n",
    "|name|object|ranked_data.csv|Name of the board game.|\n",
    "|collection_rank|int|ranked_data.csv|Ranking of the board game.|\n",
    "|geek_rating|float|ranked_data.csv|Bayesian averaged rating, reduces influence of individual ratings.|\n",
    "|avg_rating|float|ranked_data.csv|Uses user input ratings to give an average score.|\n",
    "|num_voters|int|ranked_data.csv|Number of people who gave a user rating.|\n",
    "|price|object|ranked_data.csv|Price of the game according to GeekMarket.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daac27ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to take in all data from a page and put it into a dataframe\n",
    "def page_to_data(soup):\n",
    "\n",
    "    # Create empty dataframe with desired columns\n",
    "    table_index = ['id', 'name', 'collection_rank', 'geek_rating', 'avg_rating', 'num_voters', 'price']\n",
    "    df = pd.DataFrame(index=table_index).T\n",
    "    \n",
    "    # Empty dict needs: 'id', 'name', 'collection_rank', 'geek_rating', 'avg_rating', 'num_voters', 'price'\n",
    "    # Iterate through each game\n",
    "    for game in soup.find_all(id = 'row_'):\n",
    "        data = {}\n",
    "        # Find section, get value, put it into data for all columns\n",
    "        # ID \n",
    "        temp = game.find('a', href = True)\n",
    "        data['id'] = temp.get('href').split('/')[2]\n",
    "        \n",
    "        # Name\n",
    "        temp = game.find('a', {'class': 'primary'})\n",
    "        data['name'] = temp.contents[0]\n",
    "        \n",
    "        # Collection Rank\n",
    "        temp = game.find('td', {'class': 'collection_rank'})\n",
    "        data['collection_rank'] = temp.contents[2].strip('\\t').strip('\\n').strip('\\t')\n",
    "        \n",
    "        # Geek Rating\n",
    "        temp = game.find_all('td', {'class': 'collection_bggrating'})\n",
    "        data['geek_rating'] = temp[0].contents[0].strip('\\n').strip('\\t')\n",
    "    \n",
    "        # Average Rating\n",
    "        data['avg_rating'] = temp[1].contents[0].strip('\\n').strip('\\t')\n",
    "        \n",
    "        # Number of Voters\n",
    "        data['num_voters'] = temp[2].contents[0].strip('\\n').strip('\\t')\n",
    "        \n",
    "        # Price\n",
    "        try:\n",
    "            temp = game.find('a', {'class': 'ulprice'})\n",
    "            data['price'] = temp.contents[1].contents[0]\n",
    "        except:\n",
    "            data['price'] = '(unavailable)'\n",
    "        \n",
    "        # Insert row of data in dataframe\n",
    "        df = df.append(data, ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c291a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_scrape():\n",
    "    # Empty dataframe for data insertion\n",
    "    games = pd.DataFrame()\n",
    "\n",
    "    # 50 pages -> 5000 games\n",
    "    for i in range(1, 51):\n",
    "\n",
    "        # Create selenium executable\n",
    "        base_url = 'https://boardgamegeek.com/browse/boardgame/page/'\n",
    "        fo = FirefoxOptions()\n",
    "        web = webdriver.Firefox(executable_path= '/home/clifford/Documents/geckodriver', options = fo)\n",
    "\n",
    "        # Use selenium to scrape page\n",
    "        web.get(base_url + str(i))\n",
    "        web.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        page = BeautifulSoup(web.page_source, 'lxml')\n",
    "        web.close()\n",
    "\n",
    "        # Read and insert data\n",
    "        games = games.append(page_to_data(page), ignore_index = True)\n",
    "\n",
    "        # Robots.txt says 5s, do 8 for buffer\n",
    "        time.sleep(8)\n",
    "\n",
    "    # Export to csv\n",
    "    games.to_csv('../data/ranked_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514afe2",
   "metadata": {},
   "source": [
    "## Part 2 - Categorical Data\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|id|int|cat_data.csv|Id of the game.|\n",
    "|name|object|cat_data.csv|Name of the game.| \n",
    "|year|int|cat_data.csv|Year game was published.| \n",
    "|min/max_players|int|cat_data.csv|Minimum and maximum players allowed.| \n",
    "|playtime|int|cat_data.csv|Estimated game playtime.| \n",
    "|min/max_time|int|cat_data.csv|Minimum and maximum estimated game playtime.| \n",
    "|min_age|int|cat_data.csv|Minimum recommended age.| \n",
    "|cat_#|object|cat_data.csv|Each category signifies a category or aspect of the game.|\n",
    "|categories|object|cat_data.csv|Contains a list of all of the 5 categories attributed to the game.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f64db2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to turn parsed data into understandable dictionary of data\n",
    "def parse_to_data(raw):\n",
    "    \n",
    "    # Empty dictionary\n",
    "    data = {}\n",
    "    \n",
    "    # Distinguish\n",
    "    data['id'] = raw['items']['item']['@id']\n",
    "    try:\n",
    "        data['name'] = raw['items']['item']['name'][0]['@value']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data['name'] = raw['items']['item']['name']['@value']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Get yearpublished, minplayers, maxplayers\n",
    "    data['year'] = raw['items']['item']['yearpublished']['@value']\n",
    "    data['min_players'] = raw['items']['item']['minplayers']['@value']\n",
    "    data['max_players'] = raw['items']['item']['maxplayers']['@value']\n",
    "    \n",
    "    # Get playtime, age\n",
    "    data['playtime'] = raw['items']['item']['playingtime']['@value']\n",
    "    data['min_time'] = raw['items']['item']['minplaytime']['@value']\n",
    "    data['max_time'] = raw['items']['item']['maxplaytime']['@value']\n",
    "    data['min_age'] = raw['items']['item']['minage']['@value']\n",
    "    \n",
    "    # Get board game categories and mechanics\n",
    "    mech = 1\n",
    "    fam = 1\n",
    "    for i, item in enumerate(raw['items']['item']['link']):\n",
    "\n",
    "        if item['@type'] == 'boardgamecategory':\n",
    "            data['cat_' + str(i+1)] = item['@value']\n",
    "            \n",
    "        elif item['@type'] == 'boardgamemechanic':\n",
    "            data['mech_' + str(mech)] = item['@value']\n",
    "            mech += 1\n",
    "            \n",
    "        elif item['@type'] == 'boardgamefamily':\n",
    "            data['fam_' + str(fam)] = item['@value']\n",
    "            fam += 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "#         if item['@type'] in ['boardgamecategory', 'boardgamemechanic', 'boardgamefamily']:\n",
    "#             data['cat_' + str(i+1)] = item['@value']\n",
    "    \n",
    "    # Get user poll data for suggested number of players\n",
    "    for i, item in enumerate(raw['items']['item']['poll'][0]['results']):\n",
    "        data['best_players_' + str(i+1)] = item['result'][0]['@numvotes']\n",
    "    \n",
    "    # Get user poll data for suggested age\n",
    "    for i, item in enumerate(raw['items']['item']['poll'][1]['results']['result']):\n",
    "        data['best_age_' + str(item['@value'])] = item['@numvotes']\n",
    "        \n",
    "    # Get user poll data for language dependence\n",
    "    for i, item in enumerate(raw['items']['item']['poll'][2]['results']['result']):\n",
    "        data['language_prof_' + str(item['@level'])] = item['@numvotes']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8098b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all ids given for categorical data\n",
    "def id_scrapes(ids):\n",
    "    scraped_df = pd.DataFrame(index=['id', 'name', 'year', 'min_players', 'max_players',\n",
    "                                     'playtime', 'min_time', 'max_time', 'min_age']).T\n",
    "    base_url = 'https://www.boardgamegeek.com/xmlapi2/thing?id='\n",
    "    \n",
    "    for game_id in ids:\n",
    "        res = requests.get(base_url + str(game_id))\n",
    "\n",
    "        parsed = xmltodict.parse(res.text)\n",
    "        try:\n",
    "            if parsed['items']['item']['@type'] == 'boardgame':\n",
    "                data = parse_to_data(parsed)\n",
    "                \n",
    "                # Merge\n",
    "                scraped_df = scraped_df.append(data, ignore_index = True)\n",
    "        \n",
    "                # Sleep\n",
    "                time.sleep(8)\n",
    "\n",
    "        except:\n",
    "            time.sleep(8)\n",
    "            \n",
    "    return scraped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "468241dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute scrape based on desired ranked ids\n",
    "all_id = pd.read_csv('../data/ranked_data.csv', index_col=0)\n",
    "ids = all_id['id']\n",
    "ids = ids[:2000]\n",
    "\n",
    "# Run functions to get categorical data\n",
    "# cat_df = id_scrapes(ids)\n",
    "# cat_df.to_csv('../data/cat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f4daa",
   "metadata": {},
   "source": [
    "This part of the project proved to be finicky without multiple passthroughs. To fix this, a seperate notebook titled \"1.5_Scrape_Testing\" was used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff83855",
   "metadata": {},
   "source": [
    "## Part 3 - User Ratings\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|username|object|user_ratings.csv|Name of the user reviewer.|\n",
    "|rating|int|user_ratings.csv|Rating from 1 to 10.| \n",
    "|value|object|user_ratings.csv|Comments and review NLP.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca6937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to turn web data into a dataframe with all comments in a page\n",
    "def comments_into_list(comments):\n",
    "    rating_index = ['id', 'username', 'rating', 'value']\n",
    "    test_df = pd.DataFrame(index = rating_index).T\n",
    "    for comment in comments:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['username'] = comment['@username']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data['rating'] = comment['@rating']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            data['value'] = comment['@value']\n",
    "        except:\n",
    "            pass\n",
    "        test_df = test_df.append(data, ignore_index = True)\n",
    "    return test_df\n",
    "\n",
    "def list_into_df(game_id, parsed_text):\n",
    "    # comments_into_list returns a dataframe with (generally) 100 rows * 4 columns with an empty id column\n",
    "    comment_list = parsed_text['items']['item']['comments']['comment'] # len of comment_list should be 100 until last one\n",
    "    comment_df = comments_into_list(comment_list)\n",
    "    comment_df['id'] = game_id\n",
    "    \n",
    "    return comment_df\n",
    "\n",
    "# Function to get one page of comments\n",
    "def page_ratings(game_id, page_num):\n",
    "    # Get url for use\n",
    "    base_url = 'https://www.boardgamegeek.com/xmlapi2/thing?id='\n",
    "    url = f'{base_url}{game_id}&ratingcomments=1&page={page_num}'\n",
    "    \n",
    "    # Get scraped page\n",
    "    res = requests.get(url)\n",
    "    parsed = xmltodict.parse(res.text)\n",
    "    return parsed\n",
    "\n",
    "# Function to get all ratings in a page\n",
    "def all_page_scrape(game_id):\n",
    "    # Instantiate a page counter and new dataframe\n",
    "    page = 1\n",
    "    all_comments = pd.DataFrame()\n",
    "    \n",
    "    # This while loop makes it so that the code stops when there are no more ratings to scrape\n",
    "    while 'comment' in page_ratings(game_id, page)['items']['item']['comments']:\n",
    "        all_comments = all_comments.append(list_into_df(game_id, page_ratings(game_id, page)),\n",
    "                                           ignore_index=True)\n",
    "        page += 1\n",
    "        time.sleep(8)\n",
    "    \n",
    "    return all_comments\n",
    "\n",
    "# Function to get scraped ratings of a game into a csv\n",
    "def scrape_to_csv(game_id_list):\n",
    "    \n",
    "    # Create directory for csvs if one doesn't exist\n",
    "    try:\n",
    "        os.mkdir('./game_ratings')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Scrape and export a csv for desired ids\n",
    "    for game_id in game_id_list:\n",
    "        rating_df = all_page_scrape(game_id)\n",
    "        rating_df.to_csv(f'./game_ratings/{game_id}.csv')\n",
    "\n",
    "# This code is purely decorative as the actual scraping was done remotely.\n",
    "# all_id = pd.read_csv('./ranked_data.csv', index_col=0)\n",
    "# ids = all_id['id']\n",
    "# ids = ids[:2025]\n",
    "# scrape_to_csv(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62396f9",
   "metadata": {},
   "source": [
    "The scraping for this section was done using two seperate AWS Cloud Services in order to gather the user ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
